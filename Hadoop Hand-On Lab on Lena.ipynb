{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNiKnje-Vy8D"
   },
   "source": [
    "# **Hadoop Hand-On Lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSD5nfIw4rMR"
   },
   "source": [
    "## Enable Hadoop in Jupyter Notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1642C-CiVx_m"
   },
   "outputs": [],
   "source": [
    "# set hadoop environment variable\n",
    "import os, sys\n",
    "\n",
    "os.environ[\"HADOOP_VERSION\"] = \"3.3.0\"\n",
    "os.environ[\"HADOOP_TOOLS\"] = \"/opt/hadoop/current/share/hadoop/tools/lib\"\n",
    "\n",
    "os.environ[\"WORKSPACE\"] = \"/home/<Your User Name>/hadoop\"\n",
    "\n",
    "# append hadoop executable paths to the existing system path\n",
    "%set_env PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin:/opt/hadoop/current/bin:/opt/spark/current/bin:/opt/hadoop/current/bin:/opt/mahout/current/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: We will have to export the PATHs below in order to re-run in terminal with exact commands as in our Jupyer notebook.\n",
    "\n",
    "```shell\n",
    "export HADOOP_VERSION=\"3.3.0\"\n",
    "export HADOOP_TOOLS=\"/opt/hadoop/current/share/hadoop/tools/lib\"\n",
    "export WORKSPACE=\"/home/<Your User Name>/hadoop\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QHEfG6I1zSS"
   },
   "source": [
    "## Command Line Cheat Sheet\n",
    "\n",
    "#### Accessibility\n",
    "\n",
    "All [HADOOP commands](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)  are invoked by the bin/hadoop Java script:\n",
    "```shell\n",
    "hadoop [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
    "```\n",
    "\n",
    "#### Manage files and directories\n",
    "```shell\n",
    "hadoop fs -ls -h -R # Recursively list subdirectories with human-readable file sizes.\n",
    "hadoop fs -cp  # Copy files from local to hdfs destination\n",
    "hadoop fs -mv  # Move files from source to destination\n",
    "hadoop fs -mkdir /foodir # Create a directory named /foodir\t\n",
    "hadoop fs -rm -r /foodir   # Remove a directory named /foodir\t\n",
    "hadoop fs -cat /foodir/myfile.txt #View the contents of a file named /foodir/myfile.txt\t\n",
    "```\n",
    "\n",
    "#### Transfer files between nodes\n",
    "##### put\n",
    "```shell\n",
    "hadoop fs -put [-f] [-p] [-l] [-d] [ - | <localsrc1> .. ]. <dst>\n",
    "```\n",
    "Copy single src, or multiple srcs from local file system to the destination file system. \n",
    "\n",
    "Options:\n",
    "\n",
    "    -p : Preserves rights and modification times.\n",
    "    -f : Overwrites the destination if it already exists.\n",
    "\n",
    "```shell\n",
    "hadoop fs -put localfile /user/hadoop/hadoopfile\n",
    "hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir\n",
    "```\n",
    "Similar to the fs -put command\n",
    "- `moveFromLocal` : to delete the source localsrc after copy.\n",
    "- `copyFromLocal` : source is restricted to a local file\n",
    "- `copyToLocal` : destination is restricted to a local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM5t9U_HUwl6"
   },
   "source": [
    "## **Lab 1**: Hadoop Cluster\n",
    "**Task 1.1** Check that your HDFS home directory required to execute MapReduce jobs exists.\n",
    "```bash\n",
    "hadoop fs -ls /user/${USER}\n",
    "```\n",
    "\n",
    "Type the following commands: \n",
    "```bash\n",
    "hadoop fs -ls\n",
    "hadoop fs -mkdir lab1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiZ3FvQwWOFQ"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtxgDx_LWRd0"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Vh3z-sYWUj_"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t_DSo5yWFaf"
   },
   "source": [
    "**Task 1.2** Create a folder called <code>lab1</code> and add a file <code>user.txt</code> containing your name and the date into i:\n",
    "```bash\n",
    "mkdir -p ./lab1\n",
    "echo \"FirstName LastName\" > ./lab1/user.txt\n",
    "echo `date` >> ./lab1/user.txt \n",
    "cat ./lab1/user.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zEC90JlbhG8"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQrbAm6Mbhkl"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbKmoIPWbh6Q"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_gSlslCMBqe"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6biwxwidgtz"
   },
   "source": [
    "**Task 1.3** Copy it on  HDFS :\n",
    "```bash\n",
    "hadoop fs -copyFromLocal ./lab1/user.txt lab1/.\n",
    "```\n",
    "\n",
    "Check with:\n",
    "```bash\n",
    "hadoop fs -ls -R lab1\n",
    "hadoop fs -cat lab1/user.txt \n",
    "hadoop fs -tail lab1/user.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZH8SwvSb6Yp"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNMK11FveNyH"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGmkH9SGesW8"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psKyXblaevZP"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNXFjDZvfm4r"
   },
   "source": [
    "**Task 1.4** Remove file and directory on HDFS :\n",
    "Remove the file:\n",
    "```bash\n",
    "hadoop fs -rm lab1/user.txt\n",
    "```\n",
    "Remove the directory:\n",
    "```bash\n",
    "hadoop fs -rm -r lab1\n",
    "```\n",
    "\n",
    "Check with:\n",
    "```bash\n",
    "hadoop fs -ls -R lab1 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNbm4WtPga1H"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgaajwdhgbNl"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj3rjyg2gbrd"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ythXikLqgpi8"
   },
   "source": [
    "## **Lab 2**: Command Line Hands-On Practice\n",
    "1. Create a directory <code>lab2</code> in <code>HDFS</code>.\n",
    "2. List the contents of a directory <code>lab2</code>.\n",
    "3. Upload the file <code>today.txt</code> in <code>HDFS</code>.\n",
    "```bash\n",
    "mkdir -p ./lab2\n",
    "date > ./lab2/today.txt\n",
    "whoami >> ./lab2/today.txt\n",
    "```\n",
    "4. Display contents of file <code>today.txt</code>\n",
    "5. Copy <code>today.txt</code> file from source to <code>lab2</code> directory.\n",
    "6. Copy file <code>jps.txt</code> from/To Local file system to <code>HDFS</code>. The <code>jps</code> command will report the local VM identifier for each instrumented JVM found on the target system.\n",
    "```bash\n",
    "jps > ./lab2/jps.txt\n",
    "```\n",
    "7. Move file <code>jps.txt</code> from source to <code>lab2</code>.\n",
    "8. Remove file <code>today.txt</code> from home directory in <code>HDFS</code>.\n",
    "9. Display last few lines of <code>jps.txt</code>.\n",
    "10. Display the help of <code>du</code> command and show the total amount of space in a human-readable fashion used by your home hdfs directory.\n",
    "12. Display the help of <code>df</code> command and show the total amount of space available in the filesystem in a human-readable fashion.\n",
    "13. With <code>chmod</code> change the rights of <code>today.txt</code> file. I has to be readable and writeable only by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPQ-F45_ia7D"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V6aznzqjDKd"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNNh3aC1jDkP"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8diVfJUCjDzt"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YvL_DL1jEJv"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkNmt9pDjESN"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW__nJKQjEYv"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUENrGrtjEgP"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmF7qIX9jElc"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZEk5UDZjEwp"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iT1x5xE6jE6C"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI2jeCxXjHTT"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwy6sosSn1aH"
   },
   "source": [
    "## **Lab 3**: Hadoop Streaming Using Python – Word Count Problem\n",
    "Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc. It supports all the languages that can read from standard input and write to standard output. We will be implementing Python with Hadoop Streaming and will observe how it works. We will implement the word count problem in python to understand Hadoop Streaming. We will be creating mapper.py and reducer.py to perform map and reduce tasks.\n",
    "\n",
    "Let’s create one file which contains multiple words that we can count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktg2QHynDAAQ"
   },
   "source": [
    "**Task 3.1**: Create a folder called <code>lab3</code> and add a text file with the name <code>data.txt</code> with some content into it.\n",
    "```shell\n",
    "mkdir -p ./lab3\n",
    "``` \n",
    "```shell\n",
    "%%writefile ./lab3/data.txt\n",
    "geeks for geeks is best online conding platform\n",
    "welcome to geeks for geeks hadoop streaming lab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOeRyAd_ipBg"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSSp6hKd55Kq"
   },
   "outputs": [],
   "source": [
    "%%writefile ./lab3/data.txt\n",
    "# Add Your Content Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfthZjIyFzE_"
   },
   "source": [
    "Check if <code>data.txt</code> is created in the <code>lab3</code> folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3zqqXmuP2eH"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcx7rLl7Ga1g"
   },
   "source": [
    "**Task 3.2**: Create a <code>mapper.py</code> file that implements the mapper logic. It will read the data from <code>STDIN</code> and will split the lines into words, and will generate an output of each word with its individual count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx6RQA3LPw2o"
   },
   "outputs": [],
   "source": [
    "%%file ./lab3/mapper.py\n",
    "#!/usr/bin/env python\n",
    "  \n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "  \n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # to remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "      \n",
    "    # we are looping over the words array and printing the word\n",
    "    # with the count of 1 to the STDOUT\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print(\"%s\\t%s\" % (word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xshZ7FLpHPJ1"
   },
   "source": [
    "Let’s test our mapper.py locally that it is working fine or not.\n",
    "\n",
    "***Syntax***:\n",
    "```shell\n",
    "cat <text_data_file> | python <mapper_code_python_file>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoP1SyHvPqxt"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZB41_OGIETl"
   },
   "source": [
    "**Task 3.3**: Create a <code>reducer.py</code> file that implements the reducer logic. It will read the output of <code>mapper.py</code> from <code>STDIN </code> (standard input) and will aggregate the occurrence of each word and will write the final output to <code>STDOUT</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFDzo9k2Yx8q"
   },
   "outputs": [],
   "source": [
    "%%file ./lab3/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "  \n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # splitting the data on the basis of tab we have provided in mapper.py\n",
    "    word, count = line.split(\"\\t\", 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "  \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print(\"%s\\t%s\" % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print(\"%s\\t%s\" % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_-QD3DXJCQV"
   },
   "source": [
    "Now let’s check our reducer code <code>reducer.py</code> with <code>mapper.py</code> is it working properly or not with the help of the below command.\n",
    "\n",
    "<pre>\n",
    "cat ./lab3/data.txt | python ./lab3/mapper.py | sort -k1,1 | python ./lab3/reducer.py\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4yb5FS3Y1oy"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha314NQuRHXM"
   },
   "source": [
    "**Task 3.4**: Let’s deploy our MapReduce Python code into the Hadoop environemnt.\n",
    "\n",
    "Now make a directory <code>lab3</code> in our HDFS in the root directory that will store our <code>data.txt</code> file with the below command.\n",
    "<pre>\n",
    "hadoop fs -mkdir -p lab3\n",
    "hadoop fs -mkdir -p lab3/input\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzF4sfjRSQza"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzaLtTE_JG84"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxmIyvUxSVpQ"
   },
   "source": [
    "Copy <code>data.txt</code> to this folder in our <code>HDFS</code> with help of <code>copyFromLocal</code> command.\n",
    "\n",
    "Syntax to copy a file from your local file system to the HDFS is given below:\n",
    "<pre>\n",
    "hadoop fs -copyFromLocal /path 1 /path 2 .... /path n /destination\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mySv0bVcSWQD"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7KMOacS3Rr"
   },
   "source": [
    "Now our data file has been sent to <code>HDFS</code> successfully. we can check whether it sends or not by using the below command or by manually visiting our HDFS. \n",
    "\n",
    "<pre>\n",
    "hadoop fs -ls lab3/input    # list down content of ~/lab3 directory\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blTuYhosTTIQ"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yVNWQZeTzwq"
   },
   "source": [
    "Let’s give executable permission to our <code>mapper.py</code> and <code>reducer.py</code> with the help of below command.\n",
    "<pre>\n",
    "chmod +x ./lab3/mapper.py ./lab3/reducer.py     # changing the permission to read, write, execute for user, group and others\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaTnhAgnUGBo"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_oYFU9QUT9F"
   },
   "source": [
    "Then we can observe that we have changed the file permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ7aG2Q2UXDA"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfPj_jzlU18O"
   },
   "source": [
    "**Task 3.5**: Now let’s run our python files with the help of the Hadoop streaming utility as shown below. We first create a *shell script*, called <code>wordcount.sh</code> with the content below.\n",
    "\n",
    "```shell\n",
    "hadoop jar ${HADOOP_TOOLS}/hadoop-streaming-${HADOOP_VERSION}.jar \\\n",
    "    -files ${WORKSPACE}/lab3/mapper.py,${WORKSPACE}/lab3/reducer.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input lab3/input/*.txt \\\n",
    "    -output lab3/output\n",
    "```\n",
    "\n",
    "We make <code>wordcount.sh</code> to be executable and then run the script using as follow:\n",
    "```shell\n",
    "chmod +x ./lab3/wordcount.sh\n",
    "${WORKSPACE}/lab3/wordcount.sh\n",
    "```\n",
    "\n",
    "If <code>lab3/output</code> has already existed, we will encounter error:\n",
    "```shell\n",
    "ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://lena-master/user/${USER}/lab3/output already exists\n",
    "```\n",
    "In this case, we must delete the folder before executing our <code>mapreduce</code> code.\n",
    "```shell\n",
    "hadoop fs -rm -R lab3/output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLXlmi6iHU9A"
   },
   "outputs": [],
   "source": [
    "%%file ./lab3/wordcount.sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss7aCa-IJ8TY"
   },
   "source": [
    "In the above command in <code>-output</code>, we will specify the location in <code>HDFS</code> where we want our output to be stored. So let’s check our output in output file at location <code>lab3/output/part-00000</code>. We can check results with the help of <code>cat</code> command as shown below.\n",
    "```shell\n",
    "hadoop fs -ls lab3/output\n",
    "hadoop fs -cat lab3/output/part-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H8o-R-IKvg1"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8wxxwrhKv7x"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.6**: Until now, we created a text file (2 lines), i.e. <code>data.txt</code>, and used it as an input for our MapReduce code. \n",
    "\n",
    "Now, let us use the data file, i.e. <code>books.txt</code>, specified in <code>UoL's 2.20 Programming Activity</code>. To download <code>books.txt</code> and save into <code>./lab3</code>, we can either \n",
    "\n",
    "- manually download the file to our local computer and then upload it using Jupyter notebook (or <code>ssh</code>), or\n",
    "- through <code>wget</code> with the command line below:\n",
    "```code\n",
    "wget -P ./lab3 https://learn.london.ac.uk/pluginfile.php/309878/mod_page/content/17/books.txt\n",
    "```\n",
    "\n",
    "Then, we confirm if the file is downloaded as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.7**: Copy books.txt to our HDFS lab3/input folder. We can either leave data.txt in the HDFS folder or delete. The hadoop framework is capable of processing multiple files in the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.8**: We now rerun our <code>MapReduce</code> code, but now with <code>books.txt</code> as an input. We can check results with the help of <code>cat</code> command as shown below. \n",
    "\n",
    "Note: As before, the <code>lab3/output</code> folder must not exist before we run our MapReduce code. If it does, then we must delete it before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.9**: Let us modify the MapReduce pipeline (Refer: UoL's Topic 3 Programming Activity). https://learn.london.ac.uk/pluginfile.php/309902/mod_page/content/19/Topic3_ProgrammingActivity.html?time=1605185103860\n",
    "\n",
    "Let's introduce some text normalization to the pipeline developed above. Copy your scripts to a new set (to save the originals) and add the following three features:\n",
    "\n",
    "- Discard all punctuation when counting words\n",
    "- Convert all text to lower case\n",
    "- Convert any token (ie. word) that contains one or more digits (0-9) to the string 'NOMBER', uppercase\n",
    "- Discard any words that occur less than 3 times overall\n",
    "- Discard any words that occur more than 800 times overall\n",
    "- Note Some of these features should be implemented in the mapper and some in the reducer. And remember, you can test this functionality with the serial pipeline deascribed as before.\n",
    "```shell\n",
    "cat ./lab3/data.txt | python ./lab3/mapper.py | sort -k1,1 | python ./lab3/reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.10**: Our current mapper is creating an <code>IO</code> stream of key-value (which 1) pairs, e.g. <code>(word, 1)</code> to the disk before they are sorted and sent to the reducer. The IO operations are in general expensive. For <code>WordCount</code>, we can locally reduce the results from a local mapper before they are IO streamed for the shuffle & sort phase. This process is called <code>combine</code>, which is a reduce operation which is carried out locally at the same node where the mapper is. The combiner can in overall make our MapReduce code be more efficient.\n",
    "    \n",
    "We can utilise the <code>combiner</code> feature by modifying our <code>wordcount.sh</code> as follow below.\n",
    "\n",
    "```shell\n",
    "hadoop jar ${HADOOP_TOOLS}/hadoop-streaming-${HADOOP_VERSION}.jar \\\n",
    "    -files ${WORKSPACE}/lab3/mapper.py,${WORKSPACE}/lab3/reducer.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -combiner reducer.py \\                            \n",
    "    -reducer reducer.py \\\n",
    "    -input lab3/input/*.txt \\\n",
    "    -output lab3/output\n",
    "```\n",
    "\n",
    "Observe the differences between with and without combiner MapReduce job output, particularly <code>Combine input records</code>, <code>Combine output records</code>, </code>Reduce input groups</code>, </code>Reduce shuffle bytes</code> and </code>Reduce input records</code>.\n",
    "\n",
    "The differences will be more apparent if our input data is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8yIPgNS/d6b0wgXeJtz2o",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
