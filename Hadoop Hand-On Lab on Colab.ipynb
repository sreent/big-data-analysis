{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8yIPgNS/d6b0wgXeJtz2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/big-data-analysis/blob/main/Hadoop%20Hand-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hadoop Hand-On Lab**"
      ],
      "metadata": {
        "id": "RNiKnje-Vy8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting Up Hadoop Environment"
      ],
      "metadata": {
        "id": "LSD5nfIw4rMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install java and set JAVA_HOME variable \n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# download hadoop to colab's compute engine\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "!tar -xzf hadoop-3.3.0.tar.gz\n",
        "\n",
        "# create folder for storing hadoop files\n",
        "!mkdir -p /usr/local/hadoop\n",
        "# copy downloaded files the created hadoop folder \n",
        "!cp -r hadoop-3.3.0/* /usr/local/hadoop/.\n",
        "\n",
        "# delete download files\n",
        "!rm -rf hadoop*\n",
        "\n",
        "# set java and hadoop environment\n",
        "import os, sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_VERSION\"] = \"3.3.0\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"HADOOP_CONF_DIR\"] = \"/usr/local/hadoop/etc/hadoop\"\n",
        "os.environ[\"HADOOP_MAPRED_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"HADOOP_COMMON_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"HADOOP_HDFS_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"YARN_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"HADOOP_TOOLS\"] = \"/usr/local/hadoop/share/hadoop/tools/lib\"\n",
        "\n",
        "# append hadoop executable paths to the existing system path\n",
        "%set_env PATH=/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin"
      ],
      "metadata": {
        "id": "1642C-CiVx_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Command Line Cheat Sheet\n",
        "\n",
        "####Accessibility\n",
        "\n",
        "All [HADOOP commands](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)  are invoked by the bin/hadoop Java script:\n",
        "```shell\n",
        "hadoop [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
        "```\n",
        "\n",
        "####Manage files and directories\n",
        "```shell\n",
        "hadoop fs -ls -h -R # Recursively list subdirectories with human-readable file sizes.\n",
        "hadoop fs -cp  # Copy files from local to hdfs destination\n",
        "hadoop fs -mv  # Move files from source to destination\n",
        "hadoop fs -mkdir /foodir # Create a directory named /foodir\t\n",
        "hadoop fs -rm -r /foodir   # Remove a directory named /foodir\t\n",
        "hadoop fs -cat /foodir/myfile.txt #View the contents of a file named /foodir/myfile.txt\t\n",
        "```\n",
        "\n",
        "####Transfer files between nodes\n",
        "##### put\n",
        "```shell\n",
        "hadoop fs -put [-f] [-p] [-l] [-d] [ - | <localsrc1> .. ]. <dst>\n",
        "```\n",
        "Copy single src, or multiple srcs from local file system to the destination file system. \n",
        "\n",
        "Options:\n",
        "\n",
        "    -p : Preserves rights and modification times.\n",
        "    -f : Overwrites the destination if it already exists.\n",
        "\n",
        "```shell\n",
        "hadoop fs -put localfile /user/hadoop/hadoopfile\n",
        "hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir\n",
        "```\n",
        "Similar to the fs -put command\n",
        "- `moveFromLocal` : to delete the source localsrc after copy.\n",
        "- `copyFromLocal` : source is restricted to a local file\n",
        "- `copyToLocal` : destination is restricted to a local file"
      ],
      "metadata": {
        "id": "_QHEfG6I1zSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lab 1**: Hadoop Cluster\n",
        "**Task 1.1** Check that your HDFS home directory required to execute MapReduce jobs exists.\n",
        "```bash\n",
        "hadoop fs -ls /user/${USER}\n",
        "```\n",
        "\n",
        "Type the following commands: \n",
        "```bash\n",
        "hadoop fs -ls\n",
        "hadoop fs -ls ~/\n",
        "hadoop fs -mkdir ~/lab1\n",
        "```"
      ],
      "metadata": {
        "id": "fM5t9U_HUwl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "WiZ3FvQwWOFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "QtxgDx_LWRd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "1Vh3z-sYWUj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.2** Create a folder called <code>lab1</code> and add a file <code>user.txt</code> containing your name and the date into i:\n",
        "```bash\n",
        "mkdir -p ./lab1\n",
        "echo \"FirstName LastName\" > ./lab1/user.txt\n",
        "echo `date` >> ./lab1/user.txt \n",
        "cat ./lab1/user.txt\n",
        "```"
      ],
      "metadata": {
        "id": "_t_DSo5yWFaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "6zEC90JlbhG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "mQrbAm6Mbhkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "kbKmoIPWbh6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "d_gSlslCMBqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.3** Copy it on  HDFS :\n",
        "```bash\n",
        "hadoop fs -copyFromLocal ./lab1/user.txt ~/lab1/.\n",
        "```\n",
        "\n",
        "Check with:\n",
        "```bash\n",
        "hadoop fs -ls -R ~/lab1\n",
        "hadoop fs -cat ~/lab1/user.txt \n",
        "hadoop fs -tail ~/lab1/user.txt \n",
        "```"
      ],
      "metadata": {
        "id": "w6biwxwidgtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "IZH8SwvSb6Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "iNMK11FveNyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "NGmkH9SGesW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "psKyXblaevZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.4** Remove file and directory on HDFS :\n",
        "Remove the file:\n",
        "```bash\n",
        "hadoop fs -rm ~/lab1/user.txt\n",
        "```\n",
        "Remove the directory:\n",
        "```bash\n",
        "hadoop fs -rm -r ~/lab1\n",
        "```\n",
        "\n",
        "Check with:\n",
        "```bash\n",
        "hadoop fs -ls -R ~/lab1 \n",
        "```"
      ],
      "metadata": {
        "id": "GNXFjDZvfm4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "xNbm4WtPga1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "IgaajwdhgbNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "Pj3rjyg2gbrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lab 2**: Command Line Hands-On Practice\n",
        "1. Create a directory <code>lab2</code> in <code>HDFS</code>.\n",
        "2. List the contents of a directory <code>~/lab2</code>.\n",
        "3. Upload the file <code>today.txt</code> in <code>HDFS</code>.\n",
        "```bash\n",
        "mkdir -p ./lab2\n",
        "date > ./lab2/today.txt\n",
        "whoami >> ./lab2/today.txt\n",
        "```\n",
        "4. Display contents of file <code>today.txt</code>\n",
        "5. Copy <code>today.txt</code> file from source to <code>lab2</code> directory.\n",
        "6. Copy file <code>jps.txt</code> from/To Local file system to <code>HDFS</code>. The <code>jps</code> command will report the local VM identifier for each instrumented JVM found on the target system.\n",
        "```bash\n",
        "jps > ./lab2/jps.txt\n",
        "```\n",
        "7. Move file <code>jps.txt</code> from source to <code>~/lab2</code>.\n",
        "8. Remove file <code>today.txt</code> from home directory in <code>HDFS</code>.\n",
        "9. Display last few lines of <code>jps.txt</code>.\n",
        "10. Display the help of <code>du</code> command and show the total amount of space in a human-readable fashion used by your home hdfs directory.\n",
        "12. Display the help of <code>df</code> command and show the total amount of space available in the filesystem in a human-readable fashion.\n",
        "13. With <code>chmod</code> change the rights of <code>today.txt</code> file. I has to be readable and writeable only by you."
      ],
      "metadata": {
        "id": "ythXikLqgpi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "CPQ-F45_ia7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "0V6aznzqjDKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "BNNh3aC1jDkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "8diVfJUCjDzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "5YvL_DL1jEJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "tkNmt9pDjESN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "UW__nJKQjEYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "vUENrGrtjEgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "jmF7qIX9jElc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "JZEk5UDZjEwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "iT1x5xE6jE6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "KI2jeCxXjHTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lab 3**: Hadoop Streaming Using Python – Word Count Problem\n",
        "Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc. It supports all the languages that can read from standard input and write to standard output. We will be implementing Python with Hadoop Streaming and will observe how it works. We will implement the word count problem in python to understand Hadoop Streaming. We will be creating mapper.py and reducer.py to perform map and reduce tasks.\n",
        "\n",
        "Let’s create one file which contains multiple words that we can count.\n"
      ],
      "metadata": {
        "id": "hwy6sosSn1aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.1**: Create a folder called <code>lab3</code> and add a text file with the name <code>data.txt</code> with some content into it.\n",
        "```shell\n",
        "mkdir -p ./lab3\n",
        "``` \n",
        "```shell\n",
        "%%writefile ./lab3/data.txt\n",
        "geeks for geeks is best online conding platform\n",
        "welcome to geeks for geeks hadoop streaming lab\n",
        "```"
      ],
      "metadata": {
        "id": "ktg2QHynDAAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "HOeRyAd_ipBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./lab3/data.txt\n",
        "# Add Your Content Here"
      ],
      "metadata": {
        "id": "RSSp6hKd55Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if <code>data.txt</code> is created in the <code>lab3</code> folder."
      ],
      "metadata": {
        "id": "LfthZjIyFzE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "E3zqqXmuP2eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.2**: Create a <code>mapper.py</code> file that implements the mapper logic. It will read the data from <code>STDIN</code> and will split the lines into words, and will generate an output of each word with its individual count. "
      ],
      "metadata": {
        "id": "xcx7rLl7Ga1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file ./lab3/mapper.py\n",
        "#!/usr/bin/env python\n",
        "  \n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "  \n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # to remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "      \n",
        "    # we are looping over the words array and printing the word\n",
        "    # with the count of 1 to the STDOUT\n",
        "    for word in words:\n",
        "        # write the results to STDOUT (standard output);\n",
        "        # what we output here will be the input for the\n",
        "        # Reduce step, i.e. the input for reducer.py\n",
        "        print(\"%s\\t%s\" % (word, 1))"
      ],
      "metadata": {
        "id": "tx6RQA3LPw2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s test our mapper.py locally that it is working fine or not.\n",
        "\n",
        "***Syntax***:\n",
        "```shell\n",
        "cat <text_data_file> | python <mapper_code_python_file>\n",
        "```"
      ],
      "metadata": {
        "id": "xshZ7FLpHPJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "CoP1SyHvPqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.3**: Create a <code>reducer.py</code> file that implements the reducer logic. It will read the output of <code>mapper.py</code> from <code>STDIN </code> (standard input) and will aggregate the occurrence of each word and will write the final output to <code>STDOUT</code>. "
      ],
      "metadata": {
        "id": "cZB41_OGIETl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file ./lab3/reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "  \n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "  \n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # splitting the data on the basis of tab we have provided in mapper.py\n",
        "    word, count = line.split(\"\\t\", 1)\n",
        "    # convert count (currently a string) to int\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # count was not a number, so silently\n",
        "        # ignore/discard this line\n",
        "        continue\n",
        "  \n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    # by key (here: word) before it is passed to the reducer\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            # write result to STDOUT\n",
        "            print(\"%s\\t%s\" % (current_word, current_count))\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "  \n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "    print(\"%s\\t%s\" % (current_word, current_count))"
      ],
      "metadata": {
        "id": "XFDzo9k2Yx8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s check our reducer code <code>reducer.py</code> with <code>mapper.py</code> is it working properly or not with the help of the below command.\n",
        "\n",
        "<pre>\n",
        "cat ./lab3/data.txt | python ./lab3/mapper.py | sort -k1,1 | python ./lab3/reducer.py\n",
        "</pre>"
      ],
      "metadata": {
        "id": "3_-QD3DXJCQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "F4yb5FS3Y1oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.4**: Let’s deploy our MapReduce Python code into the Hadoop environemnt.\n",
        "\n",
        "Now make a directory word_count_in_python in our HDFS in the root directory that will store our word_count_data.txt file with the below command.\n",
        "<pre>\n",
        "hadoop fs -mkdir -p ~/lab3\n",
        "hadoop fs -mkdir -p ~/lab3/input\n",
        "</pre>"
      ],
      "metadata": {
        "id": "ha314NQuRHXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "uzF4sfjRSQza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "YzaLtTE_JG84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy <code>data.txt</code> to this folder in our <code>HDFS</code> with help of <code>copyFromLocal</code> command.\n",
        "\n",
        "Syntax to copy a file from your local file system to the HDFS is given below:\n",
        "<pre>\n",
        "hadoop fs -copyFromLocal /path 1 /path 2 .... /path n /destination\n",
        "</pre>"
      ],
      "metadata": {
        "id": "mxmIyvUxSVpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "mySv0bVcSWQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data file has been sent to <code>HDFS</code> successfully. we can check whether it sends or not by using the below command or by manually visiting our HDFS. \n",
        "\n",
        "<pre>\n",
        "hadoop fs -ls ~/lab3/input    # list down content of ~/lab3 directory\n",
        "</pre>"
      ],
      "metadata": {
        "id": "ty7KMOacS3Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "blTuYhosTTIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s give executable permission to our <code>mapper.py</code> and <code>reducer.py</code> with the help of below command.\n",
        "<pre>\n",
        "chmod +x ./lab3/mapper.py ./lab3/reducer.py     # changing the permission to read, write, execute for user, group and others\n",
        "</pre>"
      ],
      "metadata": {
        "id": "4yVNWQZeTzwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "VaTnhAgnUGBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can observe that we have changed the file permission."
      ],
      "metadata": {
        "id": "U_oYFU9QUT9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "YZ7aG2Q2UXDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.5**: Now download the latest hadoop-streaming jar file from this Link. Then place, this Hadoop,-streaming jar file to a place from you can easily access it. In my case, I am placing it to /Documents folder where mapper.py and reducer.py file is present.\n",
        "\n",
        "Now let’s run our python files with the help of the Hadoop streaming utility as shown below.\n",
        "\n",
        "```shell\n",
        "hadoop jar ${HADOOP_TOOLS}/hadoop-streaming-${HADOOP_VERSION}.jar \\\n",
        "    -file  ./lab3/mapper.py  -mapper  ./lab3/mapper.py \\\n",
        "    -file  ./lab3/reducer.py -reducer ./lab3/reducer.py \\\n",
        "    -input ~/lab3/input/*.txt -output ~/lab3/output\n",
        "```"
      ],
      "metadata": {
        "id": "zfPj_jzlU18O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "fLXlmi6iHU9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above command in <code>-output</code>, we will specify the location in <code>HDFS</code> where we want our output to be stored. So let’s check our output in output file at location <code>~/lab3/output/part-00000</code>. We can check results by manually vising the location in <code>HDFS</code> or with the help of <code>cat</code> command as shown below.\n",
        "```shell\n",
        "hadoop fs -ls ~/lab3/output\n",
        "hadoop fs -cat ~/lab3/output/part-00000\n",
        "```"
      ],
      "metadata": {
        "id": "ss7aCa-IJ8TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "9H8o-R-IKvg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Insert Your Code Here"
      ],
      "metadata": {
        "id": "F8wxxwrhKv7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
